# ğŸ¤–ğŸ‡ Vision-Guided Agricultural Tasks with Diffusion-Based Motion Learning Using a Human-Robot Collaborative System

## ğŸ“‹ Pipeline Overview

<p align="center">
    <img src="./assets/images/traning_process.png" alt="Training Process" width="400"/>
    <br>
    <span align="center"><em>Pipeline for fine-tuning the pre-trained YOLOv11 nano model. Notice that only the grey box require human involved</em></span>
</p>


## ğŸš€ Getting Started

Follow these steps to set up your environment:

#### 1. ğŸ“¥ Clone the repository

```bash
git clone https://github.com/alessiodesogus/agricultural-cobot.git
cd agricultural-cobot
```

#### 2. ğŸ Create a conda environment

```bash
conda create -n agricobot python=3.10
conda activate agricobot
```

#### 3. ğŸ“¦ Install the required dependencies

```bash
pip install -r requirements.txt
```

**Dependencies included:**
- `ultralytics` - YOLOv11 implementation
- `tqdm` - Progress bars for training visualization
- `gdown` - Google Drive downloads for datasets

#### 4. ğŸ“ Download datasets and models

```bash
cd scripts/
sh yolo_datasets_download.sh
sh yolo_models_download.sh
cd ..
```

### ğŸ—‚ï¸ Data Folder Organization

Your `data/` folder should be organized as follows for proper training:

```
data/
â”œâ”€â”€ camera_1/                          # ğŸ“· Camera 1 dataset
â”‚   â”œâ”€â”€ data.yaml                      # ğŸ“„ Dataset configuration
â”‚   â”œâ”€â”€ README.dataset.txt             # ğŸ“– Dataset documentation
â”‚   â”œâ”€â”€ README.roboflow.txt            # ğŸ¤– Roboflow export info
â”‚   â”œâ”€â”€ train/                         # ğŸ‹ï¸ Training data
â”‚   â”‚   â”œâ”€â”€ images/                    # ğŸ–¼ï¸ Training images
â”‚   â”‚   â”‚   â”œâ”€â”€ IMG_001.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ IMG_002.jpg
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ labels/                    # ğŸ·ï¸ YOLO format labels
â”‚   â”‚   â”‚   â”œâ”€â”€ IMG_001.txt
â”‚   â”‚   â”‚   â”œâ”€â”€ IMG_002.txt
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ valid/                         # âœ… Validation data
â”‚   â”‚   â”œâ”€â”€ images/                    # ğŸ–¼ï¸ Validation images
â”‚   â”‚   â””â”€â”€ labels/                    # ğŸ·ï¸ Validation labels
â”‚   â””â”€â”€ test/                          # ğŸ§ª Test data
â”‚       â”œâ”€â”€ images/                    # ğŸ–¼ï¸ Test images
â”‚       â””â”€â”€ labels/                    # ğŸ·ï¸ Test labels
â””â”€â”€ camera_2/                          # ğŸ“· Camera 2 dataset
    â”œâ”€â”€ data.yaml                      # ğŸ“„ Dataset configuration
    â”œâ”€â”€ README.dataset.txt             # ğŸ“– Dataset documentation
    â”œâ”€â”€ README.roboflow.txt            # ğŸ¤– Roboflow export info
    â”œâ”€â”€ train/                         # ğŸ‹ï¸ Training data
    â”‚   â”œâ”€â”€ images/
    â”‚   â””â”€â”€labels/
    â”œâ”€â”€ valid/                         # âœ… Validation data
    â”‚   â”œâ”€â”€ images/
    â”‚   â””â”€â”€ labels/
    â””â”€â”€ test/                          # ğŸ§ª Test data
        â”œâ”€â”€ images/
        â””â”€â”€ labels/
```

- **Images**: JPG format, various resolutions supported
- **Labels**: YOLO format (.txt files) with normalized coordinates
- **Classes**: Currently configured for `Riped berries` (class 0)
- **Split**: Organized into train/valid/test folders for proper evaluation


### ğŸš‚ Training with Python Script

The Python script provides a streamlined command-line interface for training:

```bash
cd training/

# Basic training for camera 1
python yolov11n_finetuning.py --cam_id 1

# Custom parameters for camera 2
python yolov11n_finetuning.py --cam_id 2 --epochs 10 --batch_size 16 --image_size 640
```

| Parameter | Description | Default | Example |
|-----------|-------------|---------|---------|
| `--cam_id` | Camera ID (1 or 2) | Required | `--cam_id 1` |
| `--epochs` | Number of training epochs | 5 | `--epochs 25` |
| `--batch_size` | Training batch size | 8 | `--batch_size 16` |
| `--image_size` | Input image size | 1280 | `--image_size 640` |

### ğŸ“Š Training with Jupyter Notebook

For interactive development and visualization, follow the Jupyter Notebook stored at:

```bash
training/yolov11n_finetuning.ipynb
```


## ğŸ¯ Results

After training, you'll find results in:

```
runs/detect/camera_{cam_id}/
â”œâ”€â”€ yolov11n_train/                   # ğŸ‹ï¸ Training results
â”‚   â”œâ”€â”€ weights/
â”‚   â”‚   â”œâ”€â”€ best.pt                   # ğŸ† Best model weights
â”‚   â”‚   â””â”€â”€ last.pt                   # ğŸ“ Latest checkpoint
â”‚   â”œâ”€â”€ results.png                   # ğŸ“ˆ Training curves
â”‚   â”œâ”€â”€ confusion_matrix.png          # ğŸ¯ Performance matrix
â”‚   â””â”€â”€ ...
â”œâ”€â”€ yolov11n_eval/                    # âœ… Evaluation results
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ results.csv
â”‚   â””â”€â”€ ...
â””â”€â”€ yolov11n_inference/               # ğŸ”® Inference results
```

## Acknowledgments

- **[Ultralytics](https://ultralytics.com/)**: For the YOLOv11 implementation  
- **[Roboflow](https://roboflow.com/)**: For dataset management and export tools

---

ğŸŒ± **Happy farming with AI!** ğŸ¤–ğŸ‡